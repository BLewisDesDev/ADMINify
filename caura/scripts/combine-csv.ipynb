{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Combiner\n",
    "\n",
    "\n",
    "- Eliminates Duplicates without respect to Service Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 14:57:37,016 - INFO - Starting CSV merge operation in directory: /Users/byron/Downloads\n",
      "2025-05-29 14:57:37,017 - INFO - Found 15 CSV files to process\n",
      "2025-05-29 14:57:37,020 - INFO - Read 50 records from client_data_2025-05-29 (2).csv\n",
      "2025-05-29 14:57:37,022 - INFO - Read 47 records from client_data_2025-05-29 (12).csv\n",
      "2025-05-29 14:57:37,024 - INFO - Read 47 records from client_data_2025-05-29 (13).csv\n",
      "2025-05-29 14:57:37,026 - INFO - Read 49 records from client_data_2025-05-29 (3).csv\n",
      "2025-05-29 14:57:37,027 - INFO - Read 29 records from client_data_2025-05-29 (14).csv\n",
      "2025-05-29 14:57:37,033 - INFO - Read 45 records from client_data_2025-05-29 (8).csv\n",
      "2025-05-29 14:57:37,040 - INFO - Read 47 records from client_data_2025-05-29.csv\n",
      "2025-05-29 14:57:37,048 - INFO - Read 49 records from client_data_2025-05-29 (4).csv\n",
      "2025-05-29 14:57:37,056 - INFO - Read 47 records from client_data_2025-05-29 (5).csv\n",
      "2025-05-29 14:57:37,058 - INFO - Read 48 records from client_data_2025-05-29 (9).csv\n",
      "2025-05-29 14:57:37,060 - INFO - Read 49 records from client_data_2025-05-29 (6).csv\n",
      "2025-05-29 14:57:37,062 - INFO - Read 46 records from client_data_2025-05-29 (7).csv\n",
      "2025-05-29 14:57:37,066 - INFO - Read 48 records from client_data_2025-05-29 (10).csv\n",
      "2025-05-29 14:57:37,068 - INFO - Read 46 records from client_data_2025-05-29 (11).csv\n",
      "2025-05-29 14:57:37,069 - INFO - Read 48 records from client_data_2025-05-29 (1).csv\n",
      "2025-05-29 14:57:37,071 - INFO - Total records after concatenation: 695\n",
      "2025-05-29 14:57:37,073 - INFO - Found 690 unique User ID values before deduplication\n",
      "2025-05-29 14:57:37,075 - INFO - Found 10 duplicate records based on User ID\n",
      "2025-05-29 14:57:37,075 - INFO - These duplicates belong to 5 unique User ID values\n",
      "2025-05-29 14:57:37,078 - INFO - Saved duplicate records to /Users/byron/Downloads/duplicates_20250529_145737.csv for review\n",
      "2025-05-29 14:57:37,078 - INFO - Top 5 IDs with most duplicates:\n",
      "2025-05-29 14:57:37,079 - INFO -   User ID: AC81328185, Count: 2\n",
      "2025-05-29 14:57:37,079 - INFO -   User ID: AC04583001, Count: 2\n",
      "2025-05-29 14:57:37,079 - INFO -   User ID: AC58308552, Count: 2\n",
      "2025-05-29 14:57:37,080 - INFO -   User ID: AC92836188, Count: 2\n",
      "2025-05-29 14:57:37,080 - INFO -   User ID: AC75225565, Count: 2\n",
      "2025-05-29 14:57:37,081 - INFO - Removed 5 duplicate records based on User ID\n",
      "2025-05-29 14:57:37,082 - INFO - Final record count: 690\n",
      "2025-05-29 14:57:37,082 - INFO - Final unique User ID count: 690\n",
      "2025-05-29 14:57:37,086 - INFO - Successfully merged files into /Users/byron/Downloads/merged_unique_clients.csv\n",
      "2025-05-29 14:57:37,086 - INFO - Saved detailed statistics to /Users/byron/Downloads/merge_stats_20250529_145737.txt\n",
      "2025-05-29 14:57:37,087 - INFO - CSV merge operation completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def merge_csv_files(input_directory, output_filename=\"merged_output.csv\", \n",
    "                   id_column=None, keep='first', log_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files from a directory into a single CSV file,\n",
    "    removing duplicates based on ACN or client ID column.\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Path to directory containing CSV files\n",
    "        output_filename (str): Name of the output merged CSV file (default: merged_output.csv)\n",
    "        id_column (str): Column name to use for identifying unique records (ACN or client ID)\n",
    "                         If None, will try to find \"ACN\" or \"client id\" automatically\n",
    "        keep (str): Which duplicates to keep {'first', 'last', False}\n",
    "                    - 'first': Keep first occurrence of duplicates\n",
    "                    - 'last': Keep last occurrence of duplicates\n",
    "                    - False: Remove all duplicates\n",
    "                    (default: 'first')\n",
    "        log_level (int): Logging level (default: logging.INFO)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (output_path, stats_dict) where stats_dict contains detailed statistics\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    log_filename = f\"csv_merge_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    log_path = os.path.join(input_directory, log_filename)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(\"csv_merger\")\n",
    "    logger.info(f\"Starting CSV merge operation in directory: {input_directory}\")\n",
    "    \n",
    "    # Get all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(input_directory, \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        logger.error(f\"No CSV files found in {input_directory}\")\n",
    "        raise ValueError(f\"No CSV files found in {input_directory}\")\n",
    "    \n",
    "    logger.info(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    # Read and combine all CSV files\n",
    "    dfs = []\n",
    "    file_record_counts = {}\n",
    "    column_names_by_file = {}\n",
    "    \n",
    "    for file in csv_files:\n",
    "        filename = os.path.basename(file)\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            rows = len(df)\n",
    "            file_record_counts[filename] = rows\n",
    "            column_names_by_file[filename] = list(df.columns)\n",
    "            dfs.append(df)\n",
    "            logger.info(f\"Read {rows} records from {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not dfs:\n",
    "        logger.error(\"No data was read from any CSV files\")\n",
    "        raise ValueError(\"No data was read from any CSV files\")\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    total_records = len(merged_df)\n",
    "    logger.info(f\"Total records after concatenation: {total_records}\")\n",
    "    \n",
    "    # Determine the ID column if not specified\n",
    "    if id_column is None:\n",
    "        # Look for common ID column names (case-insensitive)\n",
    "        possible_id_columns = [col for col in merged_df.columns \n",
    "                             if col.upper() == \"ACN\" or col.lower() == \"client id\" \n",
    "                             or col.lower() == \"clientid\" or col.lower() == \"client_id\"\n",
    "                             or col == \"User ID\"]\n",
    "        \n",
    "        if possible_id_columns:\n",
    "            id_column = possible_id_columns[0]\n",
    "            logger.info(f\"Automatically selected '{id_column}' as the ID column\")\n",
    "        else:\n",
    "            logger.error(\"Could not automatically determine ID column. Please specify 'id_column'.\")\n",
    "            raise ValueError(\"Could not automatically determine ID column. Please specify 'id_column'.\")\n",
    "    \n",
    "    # Validate that ID column exists\n",
    "    if id_column not in merged_df.columns:\n",
    "        logger.error(f\"ID column '{id_column}' not found in the data. Available columns: {list(merged_df.columns)}\")\n",
    "        raise ValueError(f\"ID column '{id_column}' not found in the data\")\n",
    "    \n",
    "    # Check for missing values in ID column\n",
    "    missing_ids = merged_df[id_column].isna().sum()\n",
    "    if missing_ids > 0:\n",
    "        logger.warning(f\"Found {missing_ids} records with missing values in ID column '{id_column}'\")\n",
    "    \n",
    "    # Count unique IDs before deduplication\n",
    "    unique_ids_before = merged_df[id_column].nunique()\n",
    "    logger.info(f\"Found {unique_ids_before} unique {id_column} values before deduplication\")\n",
    "    \n",
    "    # Create detailed statistics\n",
    "    stats = {\n",
    "        \"original_record_count\": total_records,\n",
    "        \"file_record_counts\": file_record_counts,\n",
    "        \"column_names_by_file\": column_names_by_file,\n",
    "        \"unique_ids_before\": unique_ids_before,\n",
    "        \"id_column_used\": id_column,\n",
    "        \"records_with_missing_ids\": missing_ids\n",
    "    }\n",
    "    \n",
    "    # Find duplicates based on ID column\n",
    "    duplicates = merged_df.duplicated(subset=[id_column], keep=False)\n",
    "    duplicate_count = duplicates.sum()\n",
    "    duplicate_records = merged_df[duplicates].copy()\n",
    "    \n",
    "    # Log duplicate statistics\n",
    "    stats[\"total_duplicate_records\"] = duplicate_count\n",
    "    stats[\"duplicate_ids_count\"] = duplicate_records[id_column].nunique()\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        logger.info(f\"Found {duplicate_count} duplicate records based on {id_column}\")\n",
    "        logger.info(f\"These duplicates belong to {stats['duplicate_ids_count']} unique {id_column} values\")\n",
    "        \n",
    "        # Save duplicates to a separate file for review\n",
    "        duplicates_path = os.path.join(input_directory, f\"duplicates_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "        duplicate_records.to_csv(duplicates_path, index=False)\n",
    "        logger.info(f\"Saved duplicate records to {duplicates_path} for review\")\n",
    "        \n",
    "        # Log top 5 IDs with most duplicates\n",
    "        duplicate_counts = duplicate_records[id_column].value_counts()\n",
    "        top_duplicates = duplicate_counts.head(5)\n",
    "        logger.info(f\"Top {len(top_duplicates)} IDs with most duplicates:\")\n",
    "        for id_val, count in top_duplicates.items():\n",
    "            logger.info(f\"  {id_column}: {id_val}, Count: {count}\")\n",
    "        \n",
    "        stats[\"top_duplicate_ids\"] = {str(id_val): int(count) for id_val, count in top_duplicates.items()}\n",
    "    \n",
    "    # Remove duplicates based on ID column\n",
    "    original_count = len(merged_df)\n",
    "    merged_df = merged_df.drop_duplicates(subset=[id_column], keep=keep)\n",
    "    removed_count = original_count - len(merged_df)\n",
    "    \n",
    "    # Update statistics\n",
    "    stats[\"duplicate_records_removed\"] = removed_count\n",
    "    stats[\"final_record_count\"] = len(merged_df)\n",
    "    stats[\"unique_ids_after\"] = merged_df[id_column].nunique()\n",
    "    \n",
    "    logger.info(f\"Removed {removed_count} duplicate records based on {id_column}\")\n",
    "    logger.info(f\"Final record count: {stats['final_record_count']}\")\n",
    "    logger.info(f\"Final unique {id_column} count: {stats['unique_ids_after']}\")\n",
    "    \n",
    "    # Create output path in the same directory\n",
    "    output_path = os.path.join(input_directory, output_filename)\n",
    "    \n",
    "    # Save to output file\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    logger.info(f\"Successfully merged files into {output_path}\")\n",
    "    \n",
    "    # Create a simple stats report file\n",
    "    stats_path = os.path.join(input_directory, f\"merge_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "    with open(stats_path, 'w') as f:\n",
    "        f.write(f\"CSV Merge Operation Statistics - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Input Directory: {input_directory}\\n\")\n",
    "        f.write(f\"Output File: {output_filename}\\n\")\n",
    "        f.write(f\"ID Column Used: {id_column}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Summary Statistics:\\n\")\n",
    "        f.write(f\"  Total files processed: {len(csv_files)}\\n\")\n",
    "        f.write(f\"  Total records: {total_records}\\n\")\n",
    "        f.write(f\"  Unique {id_column} values before deduplication: {unique_ids_before}\\n\")\n",
    "        f.write(f\"  Duplicate records found: {duplicate_count}\\n\")\n",
    "        f.write(f\"  Records removed: {removed_count}\\n\")\n",
    "        f.write(f\"  Final record count: {stats['final_record_count']}\\n\")\n",
    "        f.write(f\"  Final unique {id_column} count: {stats['unique_ids_after']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"File Details:\\n\")\n",
    "        for filename, count in file_record_counts.items():\n",
    "            f.write(f\"  {filename}: {count} records\\n\")\n",
    "    \n",
    "    logger.info(f\"Saved detailed statistics to {stats_path}\")\n",
    "    logger.info(\"CSV merge operation completed successfully\")\n",
    "    \n",
    "    return output_path, stats\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your input directory\n",
    "    input_dir = \"/Users/byron/Downloads\"\n",
    "    \n",
    "    # Use specific column for identifying duplicates\n",
    "    output_path, stats = merge_csv_files(\n",
    "        input_directory=input_dir, \n",
    "        output_filename=\"merged_unique_clients.csv\",\n",
    "        id_column=\"User ID\",  # Specifically use \"User ID\" column\n",
    "        keep=\"first\",    # Keep first occurrence of each duplicate\n",
    "        log_level=logging.INFO\n",
    "    )\n",
    "    \n",
    "    # Example: Alternative ways to use the function\n",
    "    \n",
    "    # Example 1: Specify the exact ID column name\n",
    "    # output_path, stats = merge_csv_files(\n",
    "    #     input_directory=input_dir,\n",
    "    #     output_filename=\"merged_unique_acn.csv\",\n",
    "    #     id_column=\"ACN\",\n",
    "    #     keep=\"first\"\n",
    "    # )\n",
    "    \n",
    "    # Example 2: Keep last occurrence of each duplicate instead\n",
    "    # output_path, stats = merge_csv_files(\n",
    "    #     input_directory=input_dir,\n",
    "    #     output_filename=\"merged_unique_latest.csv\",\n",
    "    #     id_column=\"client id\",\n",
    "    #     keep=\"last\"\n",
    "    # )\n",
    "    \n",
    "    # Example 3: Use debug level logging for more detailed output\n",
    "    # output_path, stats = merge_csv_files(\n",
    "    #     input_directory=input_dir,\n",
    "    #     log_level=logging.DEBUG\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Combiner\n",
    "\n",
    "\n",
    "- Based new Extractor that takes TYPE from service Items\n",
    "- Sorts based on Type and then eliminates duplicates within groups not on the whole dataset\n",
    "- Used for simplified adding to DEX Client Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 16:42:38,970 - INFO - Starting CSV merge operation in directory: /Users/byron/Downloads\n",
      "2025-05-29 16:42:38,971 - INFO - Found 16 CSV files to process\n",
      "2025-05-29 16:42:38,973 - INFO - Read 47 records from client_data_2025-05-29 (2).csv\n",
      "2025-05-29 16:42:38,975 - INFO - Read 49 records from client_data_2025-05-29 (12).csv\n",
      "2025-05-29 16:42:38,977 - INFO - Read 49 records from client_data_2025-05-29 (13).csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 16:42:38,979 - INFO - Read 44 records from client_data_2025-05-29 (3).csv\n",
      "2025-05-29 16:42:38,986 - INFO - Read 47 records from client_data_2025-05-29 (14).csv\n",
      "2025-05-29 16:42:38,998 - INFO - Read 47 records from client_data_2025-05-29 (8).csv\n",
      "2025-05-29 16:42:39,006 - INFO - Read 42 records from client_data_2025-05-29.csv\n",
      "2025-05-29 16:42:39,008 - INFO - Read 48 records from client_data_2025-05-29 (4).csv\n",
      "2025-05-29 16:42:39,011 - INFO - Read 50 records from client_data_2025-05-29 (5).csv\n",
      "2025-05-29 16:42:39,013 - INFO - Read 47 records from client_data_2025-05-29 (9).csv\n",
      "2025-05-29 16:42:39,015 - INFO - Read 25 records from client_data_2025-05-29 (15).csv\n",
      "2025-05-29 16:42:39,016 - INFO - Read 45 records from client_data_2025-05-29 (6).csv\n",
      "2025-05-29 16:42:39,018 - INFO - Read 48 records from client_data_2025-05-29 (7).csv\n",
      "2025-05-29 16:42:39,020 - INFO - Read 49 records from client_data_2025-05-29 (10).csv\n",
      "2025-05-29 16:42:39,021 - INFO - Read 50 records from client_data_2025-05-29 (11).csv\n",
      "2025-05-29 16:42:39,023 - INFO - Read 47 records from client_data_2025-05-29 (1).csv\n",
      "2025-05-29 16:42:39,024 - INFO - Total records after concatenation: 734\n",
      "2025-05-29 16:42:39,025 - INFO - Found 727 unique ACN values before deduplication\n",
      "2025-05-29 16:42:39,027 - INFO - Sorted data by Type (HM first, then DA)\n",
      "2025-05-29 16:42:39,028 - INFO - Found 1 distinct service types: HM\n",
      "2025-05-29 16:42:39,029 - INFO - In HM: Found 14 duplicate records for 7 unique ACN values\n",
      "2025-05-29 16:42:39,030 - INFO - In HM: Removed 7 duplicate records\n",
      "2025-05-29 16:42:39,031 - INFO - Saved duplicate records to /Users/byron/Downloads/duplicates_20250529_164239.csv for review\n",
      "2025-05-29 16:42:39,032 - INFO - Final record count: 727\n",
      "2025-05-29 16:42:39,032 - INFO - Final unique ACN count: 727\n",
      "2025-05-29 16:42:39,033 - INFO - Service type HM: 727 records\n",
      "2025-05-29 16:42:39,036 - INFO - Successfully merged files into /Users/byron/Downloads/merged_unique_clients.csv\n",
      "2025-05-29 16:42:39,036 - INFO - Saved detailed statistics to /Users/byron/Downloads/merge_stats_20250529_164239.txt\n",
      "2025-05-29 16:42:39,037 - INFO - CSV merge operation completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def merge_csv_files(input_directory, output_filename=\"merged_output.csv\", \n",
    "                   id_column=None, type_column=\"Type\", keep='last', log_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files from a directory into a single CSV file,\n",
    "    removing duplicates based on ACN or client ID column, but only within same type group.\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Path to directory containing CSV files\n",
    "        output_filename (str): Name of the output merged CSV file (default: merged_output.csv)\n",
    "        id_column (str): Column name to use for identifying unique records (ACN or client ID)\n",
    "                         If None, will try to find \"ACN\" or \"client id\" automatically\n",
    "        type_column (str): Column name for the service type (default: \"Type\")\n",
    "        keep (str): Which duplicates to keep {'first', 'last', False}\n",
    "                    - 'first': Keep first occurrence of duplicates\n",
    "                    - 'last': Keep last occurrence of duplicates\n",
    "                    - False: Remove all duplicates\n",
    "                    (default: 'first')\n",
    "        log_level (int): Logging level (default: logging.INFO)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (output_path, stats_dict) where stats_dict contains detailed statistics\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    log_filename = f\"csv_merge_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    log_path = os.path.join(input_directory, log_filename)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(\"csv_merger\")\n",
    "    logger.info(f\"Starting CSV merge operation in directory: {input_directory}\")\n",
    "    \n",
    "    # Get all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(input_directory, \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        logger.error(f\"No CSV files found in {input_directory}\")\n",
    "        raise ValueError(f\"No CSV files found in {input_directory}\")\n",
    "    \n",
    "    logger.info(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    # Read and combine all CSV files\n",
    "    dfs = []\n",
    "    file_record_counts = {}\n",
    "    column_names_by_file = {}\n",
    "    \n",
    "    for file in csv_files:\n",
    "        filename = os.path.basename(file)\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            rows = len(df)\n",
    "            file_record_counts[filename] = rows\n",
    "            column_names_by_file[filename] = list(df.columns)\n",
    "            dfs.append(df)\n",
    "            logger.info(f\"Read {rows} records from {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not dfs:\n",
    "        logger.error(\"No data was read from any CSV files\")\n",
    "        raise ValueError(\"No data was read from any CSV files\")\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    total_records = len(merged_df)\n",
    "    logger.info(f\"Total records after concatenation: {total_records}\")\n",
    "    \n",
    "    # Determine the ID column if not specified\n",
    "    if id_column is None:\n",
    "        # Look for common ID column names (case-insensitive)\n",
    "        possible_id_columns = [col for col in merged_df.columns \n",
    "                             if col.upper() == \"ACN\" or col.lower() == \"client id\" \n",
    "                             or col.lower() == \"clientid\" or col.lower() == \"client_id\"\n",
    "                             or col == \"User ID\"]\n",
    "        \n",
    "        if possible_id_columns:\n",
    "            id_column = possible_id_columns[0]\n",
    "            logger.info(f\"Automatically selected '{id_column}' as the ID column\")\n",
    "        else:\n",
    "            logger.error(\"Could not automatically determine ID column. Please specify 'id_column'.\")\n",
    "            raise ValueError(\"Could not automatically determine ID column. Please specify 'id_column'.\")\n",
    "    \n",
    "    # Validate that ID column exists\n",
    "    if id_column not in merged_df.columns:\n",
    "        logger.error(f\"ID column '{id_column}' not found in the data. Available columns: {list(merged_df.columns)}\")\n",
    "        raise ValueError(f\"ID column '{id_column}' not found in the data\")\n",
    "    \n",
    "    # Validate that Type column exists\n",
    "    if type_column not in merged_df.columns:\n",
    "        logger.error(f\"Type column '{type_column}' not found in the data. Available columns: {list(merged_df.columns)}\")\n",
    "        raise ValueError(f\"Type column '{type_column}' not found in the data\")\n",
    "    \n",
    "    # Check for missing values in ID column\n",
    "    missing_ids = merged_df[id_column].isna().sum()\n",
    "    if missing_ids > 0:\n",
    "        logger.warning(f\"Found {missing_ids} records with missing values in ID column '{id_column}'\")\n",
    "    \n",
    "    # Count unique IDs before deduplication\n",
    "    unique_ids_before = merged_df[id_column].nunique()\n",
    "    logger.info(f\"Found {unique_ids_before} unique {id_column} values before deduplication\")\n",
    "    \n",
    "    # Sort by Type (HM, DA) - ensure HM types come first\n",
    "    merged_df['sort_order'] = merged_df[type_column].apply(lambda x: 0 if x == 'HM' else 1 if x == 'DA' else 2)\n",
    "    merged_df.sort_values(by=['sort_order', id_column], inplace=True)\n",
    "    merged_df.drop('sort_order', axis=1, inplace=True)\n",
    "    \n",
    "    logger.info(f\"Sorted data by {type_column} (HM first, then DA)\")\n",
    "    \n",
    "    # Create detailed statistics\n",
    "    stats = {\n",
    "        \"original_record_count\": total_records,\n",
    "        \"file_record_counts\": file_record_counts,\n",
    "        \"column_names_by_file\": column_names_by_file,\n",
    "        \"unique_ids_before\": unique_ids_before,\n",
    "        \"id_column_used\": id_column,\n",
    "        \"type_column_used\": type_column,\n",
    "        \"records_with_missing_ids\": missing_ids\n",
    "    }\n",
    "    \n",
    "    # Group by type and check for duplicates within each type\n",
    "    type_groups = merged_df[type_column].unique()\n",
    "    logger.info(f\"Found {len(type_groups)} distinct service types: {', '.join(map(str, type_groups))}\")\n",
    "    \n",
    "    all_duplicates = pd.DataFrame()\n",
    "    total_dupes_removed = 0\n",
    "    \n",
    "    # Process each type group separately\n",
    "    deduped_dfs = []\n",
    "    for service_type in type_groups:\n",
    "        type_df = merged_df[merged_df[type_column] == service_type].copy()\n",
    "        type_count_before = len(type_df)\n",
    "        \n",
    "        # Find duplicates based on ID column within this type\n",
    "        type_duplicates = type_df.duplicated(subset=[id_column], keep=False)\n",
    "        type_duplicate_count = type_duplicates.sum()\n",
    "        \n",
    "        if type_duplicate_count > 0:\n",
    "            duplicate_records = type_df[type_duplicates].copy()\n",
    "            all_duplicates = pd.concat([all_duplicates, duplicate_records])\n",
    "            \n",
    "            # Count of unique IDs with duplicates in this type\n",
    "            duplicate_ids_count = duplicate_records[id_column].nunique()\n",
    "            logger.info(f\"In {service_type}: Found {type_duplicate_count} duplicate records for {duplicate_ids_count} unique {id_column} values\")\n",
    "            \n",
    "            # Remove duplicates\n",
    "            type_df = type_df.drop_duplicates(subset=[id_column], keep=keep)\n",
    "            removed_count = type_count_before - len(type_df)\n",
    "            total_dupes_removed += removed_count\n",
    "            \n",
    "            logger.info(f\"In {service_type}: Removed {removed_count} duplicate records\")\n",
    "        else:\n",
    "            logger.info(f\"In {service_type}: No duplicates found\")\n",
    "        \n",
    "        deduped_dfs.append(type_df)\n",
    "    \n",
    "    # Combine the deduplicated dataframes\n",
    "    merged_df = pd.concat(deduped_dfs, ignore_index=True)\n",
    "    \n",
    "    # Save duplicates to a separate file if any were found\n",
    "    if not all_duplicates.empty:\n",
    "        duplicates_path = os.path.join(input_directory, f\"duplicates_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "        all_duplicates.to_csv(duplicates_path, index=False)\n",
    "        logger.info(f\"Saved duplicate records to {duplicates_path} for review\")\n",
    "    \n",
    "    # Update statistics\n",
    "    stats[\"duplicate_records_removed\"] = total_dupes_removed\n",
    "    stats[\"final_record_count\"] = len(merged_df)\n",
    "    stats[\"unique_ids_after\"] = merged_df[id_column].nunique()\n",
    "    \n",
    "    # Count records by type\n",
    "    type_counts = merged_df[type_column].value_counts().to_dict()\n",
    "    stats[\"records_by_type\"] = type_counts\n",
    "    \n",
    "    logger.info(f\"Final record count: {stats['final_record_count']}\")\n",
    "    logger.info(f\"Final unique {id_column} count: {stats['unique_ids_after']}\")\n",
    "    for type_val, count in type_counts.items():\n",
    "        logger.info(f\"Service type {type_val}: {count} records\")\n",
    "    \n",
    "    # Count clients with both service types\n",
    "    if 'HM' in type_counts and 'DA' in type_counts:\n",
    "        hm_clients = set(merged_df[merged_df[type_column] == 'HM'][id_column])\n",
    "        da_clients = set(merged_df[merged_df[type_column] == 'DA'][id_column])\n",
    "        clients_with_both = hm_clients.intersection(da_clients)\n",
    "        \n",
    "        stats[\"clients_with_both_service_types\"] = len(clients_with_both)\n",
    "        logger.info(f\"Clients with both HM and DA services: {len(clients_with_both)}\")\n",
    "    \n",
    "    # Create output path in the same directory\n",
    "    output_path = os.path.join(input_directory, output_filename)\n",
    "    \n",
    "    # Save to output file\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    logger.info(f\"Successfully merged files into {output_path}\")\n",
    "    \n",
    "    # Create a simple stats report file\n",
    "    stats_path = os.path.join(input_directory, f\"merge_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "    with open(stats_path, 'w') as f:\n",
    "        f.write(f\"CSV Merge Operation Statistics - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Input Directory: {input_directory}\\n\")\n",
    "        f.write(f\"Output File: {output_filename}\\n\")\n",
    "        f.write(f\"ID Column Used: {id_column}\\n\")\n",
    "        f.write(f\"Type Column Used: {type_column}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Summary Statistics:\\n\")\n",
    "        f.write(f\"  Total files processed: {len(csv_files)}\\n\")\n",
    "        f.write(f\"  Total records: {total_records}\\n\")\n",
    "        f.write(f\"  Unique {id_column} values before deduplication: {unique_ids_before}\\n\")\n",
    "        f.write(f\"  Duplicate records removed: {total_dupes_removed}\\n\")\n",
    "        f.write(f\"  Final record count: {stats['final_record_count']}\\n\")\n",
    "        f.write(f\"  Final unique {id_column} count: {stats['unique_ids_after']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Records by Type:\\n\")\n",
    "        for type_val, count in type_counts.items():\n",
    "            f.write(f\"  {type_val}: {count} records\\n\")\n",
    "        \n",
    "        if 'clients_with_both_service_types' in stats:\n",
    "            f.write(f\"\\nClients with both HM and DA services: {stats['clients_with_both_service_types']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"File Details:\\n\")\n",
    "        for filename, count in file_record_counts.items():\n",
    "            f.write(f\"  {filename}: {count} records\\n\")\n",
    "    \n",
    "    logger.info(f\"Saved detailed statistics to {stats_path}\")\n",
    "    logger.info(\"CSV merge operation completed successfully\")\n",
    "    \n",
    "    return output_path, stats\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your input directory\n",
    "    input_dir = \"/Users/byron/Downloads\"\n",
    "    \n",
    "\t\n",
    "    # Use specific column for identifying duplicates\n",
    "    output_path, stats = merge_csv_files(\n",
    "        input_directory=input_dir, \n",
    "        output_filename=\"merged_unique_clients.csv\",\n",
    "        id_column=\"ACN\",    # Specifically use \"User ID\" column\n",
    "        type_column=\"Type\",     # Service type column\n",
    "        keep=\"first\",           # Keep first occurrence of each duplicate within type\n",
    "        log_level=logging.INFO\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
